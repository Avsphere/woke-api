{"authors":["Mark Prigg For Dailymail.Com"],"date_download":"2018-10-23 03:54:11","date_modify":"2018-10-23 03:54:11","date_publish":"2016-01-29 19:20:09","description":"Researchers were able to predict what people are seeing based on the electrical signals coming from electrodes implanted in their brain - and say it could allow 'locked in' patients to communicate.","filename":"data/vaneerConsumption/MindreadingcomputerINSTANTLYknowswhatyourethinkingabout.json","image_url":"https://i.dailymail.co.uk/i/pix/2016/01/29/19/30B4E01F00000578-0-image-a-18_1454095137980.jpg","language":"en","localpath":"/home/avsp_here/news-please-repo//data/2018/10/22/dailymail.co.uk/sciencetech_article-3423290_Mind-reading-computer-INSTANTLY-knows-thinking-about_1540266851.html","title":"Mind-reading computer INSTANTLY knows what you're thinking about","title_page":"Mind-reading computer INSTANTLY knows what you're thinking about | Daily Mail Online","title_rss":"NULL","source_domain":"dailymail.co.uk","text":"A radical new computer program can decode people's thoughts almost in real time, researchers have claimed.\nThey were able to predict what people are seeing based on the electrical signals coming from electrodes implanted in their brain - and say it could allow 'locked in' patients to communicate.\nThe decoding happens within milliseconds of someone first seeing the image, and had better than 95 percent accuracy, the scientists said.\nScroll down for video\nSubjects viewed a random sequence of images of faces and houses and were asked to look for an inverted house like the one at bottom left. The decoding happens within milliseconds of someone first seeing the image, and had better than 95 percent accuracy, the scientists said.\nWHAT IS THE TEMPORAL LOBE? Temporal lobes process sensory input and are a common site of epileptic seizures. Situated behind mammals' eyes and ears, the lobes are also involved in Alzheimer's and dementias and appear somewhat more vulnerable than other brain structures to head traumas, he said.\n'We were trying to understand, first, how the human brain perceives objects in the temporal lobe, and second, how one could use a computer to extract and predict what someone is seeing in real time?' said University of Washington computational neuroscientist Rajesh Rao.\n'Clinically, you could think of our result as a proof of concept toward building a communication mechanism for patients who are paralyzed or have had a stroke and are completely locked-in,' he said.\nResearchers used electrodes implanted in the temporal lobes of awake patients,.\nFurther, analysis of patients' neural responses to two categories of visual stimuli – images of faces and houses – enabled the scientists to subsequently predict which images the patients were viewing, and when, with better than 95 percent accuracy.\nThe research is published today in PLOS Computational Biology.\nThe study involved seven epilepsy patients receiving care at Harborview Medical Center in Seattle.\nEach was experiencing epileptic seizures not relieved by medication, said UW Medicine neurosurgeon Jeff Ojemann, so each had undergone surgery in which their brains' temporal lobes were implanted – temporarily, for about a week – with electrodes to try to locate the seizures' focal points.\n'They were going to get the electrodes no matter what; we were just giving them additional tasks to do during their hospital stay while they are otherwise just waiting around,' Ojemann said.\nTemporal lobes process sensory input and are a common site of epileptic seizures.\nSituated behind mammals' eyes and ears, the lobes are also involved in Alzheimer's and dementias and appear somewhat more vulnerable than other brain structures to head traumas, he said.\nThe subjects, watching a computer monitor, were shown a random sequence of pictures – brief (400 millisecond) flashes of images of human faces and houses, interspersed with blank gray screens. Their task was to watch for an image of an upside-down house. The numbers 1-4 denote electrode placement in temporal lobe, and neural responses of two signal types being measured.\nIn the experiment, the electrodes from multiple temporal-lobe locations were connected to powerful computational software that extracted two characteristic properties of the brain signal: 'event-related potentials' and 'broadband spectral changes.'\nRao characterized the former as likely arising from 'hundreds of thousands of neurons being co-activated when an image is first presented,' and the latter as 'continued processing after the initial wave of information.'\nThe subjects, watching a computer monitor, were shown a random sequence of pictures – brief (400 millisecond) flashes of images of human faces and houses, interspersed with blank gray screens. Their task was to watch for an image of an upside-down house.\n'We got different responses from different (electrode) locations; some were sensitive to faces and some were sensitive to houses,' Rao said.\nTHE JAPANESE MACHINE THAT COULD ALLOW TELEPATHIC TALK A 'mind-reading' device that can decipher words from brainwaves without them being spoken has been developed by Japanese scientists, raising the prospect of 'telepathic' communication. Researchers have found the electrical activity in the brain is the same when words are spoken and when they are left unsaid. By looking for the distinct wave forms produced before speaking, the team was able to identify words such as 'goo', 'scissors' and 'par' when spoken in Japanese. Researchers from Japan used technology that measures the electrical activity of the brain to decipher brainwaves that occur before someone speaks (stock picture). They found distinct brainwaves were formed before syllables were spoken\nThe computational software sampled and digitized the brain signals 1,000 times per second to extract their characteristics.\nThe software also analyzed the data to determine which combination of electrode locations and signal types correlated best with what each subject actually saw.\nIn that way it yielded highly predictive information.\nBy training an algorithm on the subjects' responses to the (known) first two-thirds of the images, the researchers could examine the brain signals representing the final third of the images, whose labels were unknown to them, and predict with 96 percent accuracy whether and when (within 20 milliseconds) the subjects were seeing a house, a face or a gray screen.\nThis accuracy was attained only when event-related potentials and broadband changes were combined for prediction, which suggests they carry complementary information.\n'Traditionally scientists have looked at single neurons,' Rao said. 'Our study gives a more global picture, at the level of very large networks of neurons, of how a person who is awake and paying attention perceives a complex visual object.'\nThe scientists' technique, he said, is a steppingstone for brain mapping, in that it could be used to identify in real time which locations of the brain are sensitive to particular types of information.\n'The computational tools that we developed can be applied to studies of motor function, studies of epilepsy, studies of memory.","url":"https://www.dailymail.co.uk/sciencetech/article-3423290/Mind-reading-computer-INSTANTLY-knows-thinking-about.html"}
